import os, json, sys, io, shutil, re
from bs4 import BeautifulSoup
import requests
import asyncio
from playwright.async_api import async_playwright
from fastmcp import FastMCP
from google import genai
from google.genai import types
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import PydanticOutputParser
from typing import List, Optional
from pydantic import BaseModel, Field

sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')

# ==========================================
# 1. API í‚¤ ì„¤ì • (ë³¸ì¸ì˜ OpenAI í‚¤ë¡œ êµì²´ í•„ìˆ˜!)
# ==========================================
GEMINI_API_KEY = "API_KEY"
client = genai.Client(api_key=GEMINI_API_KEY)
MODEL_ID = "gemini-3-flash-preview"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ==========================================
# 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ (AIê°€ ì±„ì›Œì•¼ í•  ì •ë‹µì§€)
# ==========================================
class PromotionItem(BaseModel):
    product_name: str = Field(description="ìƒí’ˆëª…")
    final_price: int = Field(description="í‘œì‹œëœ ìµœì¢… íŒë§¤ê°€ (ìˆ«ìë§Œ)")
    # --- ìƒˆë¡œ ì¶”ê°€í•  í•„ë“œ ---
    unit_price: int = Field(description="1ê°œë‹¹ ì‹¤ì§ˆ êµ¬ë§¤ ê°€ê²© (1+1ì´ë©´ final_priceì˜ ì ˆë°˜)")
    # -----------------------
    original_price: int = Field(description="ì •ìƒê°€")
    discount_condition: str = Field(description="í• ì¸ ì¡°ê±´ (ì˜ˆ: 1+1, 2ê°œ êµ¬ë§¤ì‹œ 50% ë“±)")
    unit: str = Field(description="íŒë§¤ ë‹¨ìœ„")

class StoreFlyerAnalysis(BaseModel):
    store_name: str = Field(description="í¸ì˜ì /ë§ˆíŠ¸ ì´ë¦„")
    items: List[PromotionItem] = Field(description="í–‰ì‚¬ ìƒí’ˆ ëª©ë¡")
    summary: str = Field(description="ì „ì²´ í–‰ì‚¬ ìš”ì•½ (3ì¤„ ì´ë‚´)")

# ==========================================
# 3. ì„œë²„ ë° LangChain ì„¤ì •
# ==========================================
mcp = FastMCP("Convenience Store Vision Bot")

# íŒŒì„œ ì„¤ì • (Pydantic ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ íŒŒì‹±)
parser = PydanticOutputParser(pydantic_object=StoreFlyerAnalysis)

def save_to_db(store_name: str, items: list):
    """ìˆ˜ì§‘ëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œì»¬ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    # íŒŒì¼ëª…ì„ db_cu.json, db_emart.json ì‹ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    file_path = f"db_{store_name.lower()}.json"
    
    data_to_save = {
        "store_name": store_name,
        "last_updated": "2025-11-20", # ë‚ ì§œë¥¼ í•˜ë“œì½”ë”©í•˜ê±°ë‚˜ datetimeì„ ì“°ì„¸ìš”
        "total_count": len(items),
        "items": items
    }
    
    with open(file_path, "w", encoding="utf-8") as f:
        # indent=2ë¥¼ ì£¼ë©´ ë©”ëª¨ì¥ìœ¼ë¡œ ì—´ì—ˆì„ ë•Œ ì˜ˆì˜ê²Œ ë³´ì…ë‹ˆë‹¤.
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"[SUCCESS] {file_path} ì €ì¥ ì™„ë£Œ! (ì´ {len(items)}ê°œ)")

def normalize_product_data(item: dict) -> dict:
    """
    ìƒí’ˆëª…, í–‰ì‚¬ë‚´ìš©, ë‹¨ìœ„ í•„ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ 
    ìš©ëŸ‰(capacity_ml)ê³¼ 100ë‹¨ìœ„ë‹¹ ê°€ê²©(unit_price_per_100)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    p_name = item.get("product_name", "")
    price = item.get("final_price", 0)
    condition = item.get("discount_condition", "")
    unit_field = item.get("unit", "")
    
    # 1. íƒìƒ‰í•  í…ìŠ¤íŠ¸ í›„ë³´êµ° (ìˆœì„œ ì¤‘ìš”: ìƒí’ˆëª… -> í–‰ì‚¬ë‚´ìš© -> ë‹¨ìœ„)
    # Noneì´ ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    search_targets = [
        str(p_name), 
        str(condition), 
        str(unit_field)
    ]
    
    capacity = 0
    
    # ì •ê·œì‹: ì†Œìˆ˜ì  ì§€ì› (1.1kg), ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
    # ì˜ˆ: 1.5L, 200ml, 500g, 1kg
    pattern = r'(\d+(?:\.\d+)?)\s*(ml|l|g|kg)'
    
    for text in search_targets:
        match = re.search(pattern, text.lower())
        if match:
            value = float(match.group(1))
            unit = match.group(2)
            
            # ë‹¨ìœ„ ë³€í™˜ (L, kg -> 1000ë°°)
            if unit in ['l', 'kg']:
                capacity = int(value * 1000)
            else:
                capacity = int(value)
            
            # ë¬¶ìŒ ìƒí’ˆ ì²´í¬ (x3, *3ì… ë“±) - í•´ë‹¹ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì°¾ê¸°
            bundle_match = re.search(r'[\*x]\s*(\d+)', text.lower())
            if bundle_match:
                count = int(bundle_match.group(1))
                capacity *= count
            
            # ìš©ëŸ‰ì„ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ ì¤‘ë‹¨ (ë” ì´ìƒ ë’¤ì§ˆ í•„ìš” ì—†ìŒ)
            break

    # 2. ì‹¤ì§ˆ ê°€ê²© ë° ìš©ëŸ‰ ê³„ì‚° (í–‰ì‚¬ ë°˜ì˜)
    total_capacity = capacity
    pay_price = price
    
    # í–‰ì‚¬ ë‚´ìš©(condition)ì€ ì–´ë””ì„œ ìš©ëŸ‰ì„ ì°¾ì•˜ë“  í•­ìƒ ì°¸ì¡°í•´ì•¼ í•¨
    cond_lower = str(condition).lower()
    
    if "1+1" in cond_lower:
        total_capacity = capacity * 2
    elif "2+1" in cond_lower:
        total_capacity = capacity * 3
        pay_price = price * 2

    # 3. ë°ì´í„° ì£¼ì…
    if total_capacity > 0:
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        item["unit_price_per_100"] = int((pay_price / total_capacity) * 100)
        item["capacity_ml"] = capacity
    else:
        # ìš©ëŸ‰ íŒŒì•… ë¶ˆê°€ ì‹œ
        item["unit_price_per_100"] = 0
        item["capacity_ml"] = 0
        
    return item

def normalize_to_list(data):
        """ë°ì´í„°ê°€ ë¬´ì—‡ì´ë“  'ì†Œë¬¸ì ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸'ë¡œ ë³€í™˜í•˜ëŠ” ë°©ì–´ í•¨ìˆ˜"""
        if isinstance(data, list):
            # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ ìš”ì†Œë“¤ì„ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë°”ê¾¸ê³  ì†Œë¬¸ìí™” (None ë“± ë°©ì–´)
            return [str(i).lower().strip() for i in data if i]
        if isinstance(data, str):
            # ë‹¨ì¼ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê³  ì†Œë¬¸ìí™”
            return [data.lower().strip()]
        return []

async def analyze_text_with_llm(mart_name: str, raw_text: str) -> str:
    """ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜ë“œì‹œ 'items' í‚¤ë¥¼ í¬í•¨í•œ JSONì„ ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤."""
    
    # Pydantic íŒŒì„œì˜ ì§€ì‹œì‚¬í•­ì„ í¬í•¨í•˜ì—¬ í˜•ì‹ì„ ê°•ì œí•©ë‹ˆë‹¤.
    format_instructions = parser.get_format_instructions()
    
    prompt_text = f"""
    ë‹¹ì‹ ì€ {mart_name}ì˜ ì „ë‹¨ì§€ ë°ì´í„° ì •ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ìƒí’ˆ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
    
    [ì¤‘ìš”: ì´ë¯¸ì§€ URL ì²˜ë¦¬]
    - ì…ë ¥ í…ìŠ¤íŠ¸ì— ìˆëŠ” ì´ë¯¸ì§€ ì£¼ì†Œ(http...)ë¥¼ 'image_url' í•„ë“œì— ê·¸ëŒ€ë¡œ ë„£ìœ¼ì„¸ìš”.
    - ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‘ì„¸ìš”.
    
    [í•„ìˆ˜ êµ¬ì¡°]
    {{
      "store_name": "{mart_name}",
      "items": [
        {{
          "product_name": "ìƒí’ˆëª…",
          "final_price": 10000,
          "original_price": 12000,
          "discount_condition": "1+1",
          "unit": "ê°œ/ì…",
          "image_url": "" 
        }}
      ],
      "summary": "ìš”ì•½"
    }}
    
    [ë°ì´í„°]
    {raw_text}
    
    {format_instructions}
    """
    
    # ë¹„ë™ê¸°ë¡œ Gemini í˜¸ì¶œ
    response = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=prompt_text,
        config=types.GenerateContentConfig(
            response_mime_type="application/json",
            temperature=0.1
        )
    )
    
    # response_mime_type: "application/json" ì„¤ì • ë•ë¶„ì— ë°”ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ë„ ë©ë‹ˆë‹¤.
    return response.text

async def _get_tags_logic(product_names: List[str]) -> str:
    # LLMì´ ì›ë˜ ì´ë¦„ì„ ìœ ì§€í•˜ê²Œ í•˜ê³ , JSON êµ¬ì¡°ë¥¼ ëª…í™•íˆ ì§€ì •í•©ë‹ˆë‹¤.
    prompt = f"""
    ë„ˆëŠ” í¸ì˜ì  ìƒí’ˆ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì„œ íƒœê·¸ë¥¼ ë‹¬ì•„ì¤˜.
    
    [ì§€ì‹œ ì‚¬í•­]
    1. ê° ìƒí’ˆë§ˆë‹¤ ë°˜ë“œì‹œ "product_name" í•„ë“œì— ì œê³µëœ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ë„£ì–´ì¤˜. (ë§¤ì¹­ì„ ìœ„í•´ í•„ìˆ˜)
    2. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì§€í‚¨ JSON ë°°ì—´ì´ì–´ì•¼ í•´.
    
    [ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ]
    [
      {{
        "product_name": "ì›ë³¸ìƒí’ˆëª…",
        "brand": "ë¸Œëœë“œ",
        "category": "ì¹´í…Œê³ ë¦¬",
        "taste": "ë§›1, ë§›2",
        "situation": "ìƒí™©1, ìƒí™©2",
        "target": "íƒ€ê²Ÿ"
      }}
    ]

    [ë¶„ì„í•  ìƒí’ˆ ë¦¬ìŠ¤íŠ¸]: {", ".join(product_names)}
    """
    
    response = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=prompt,
        config=types.GenerateContentConfig(
            # JSON ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ì  ë¬´ê²°ì„± ë³´ì¥
            response_mime_type="application/json",
            temperature=0.1
        )
    )
    return response.text

# 2. MCPê°€ ì¸ì‹í•˜ëŠ” ë„êµ¬ (ìœ„ì˜ ìˆœìˆ˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨)
@mcp.tool()
async def get_tags_from_llm(product_names: List[str]) -> str:
    """ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ AI íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return await _get_tags_logic(product_names)

@mcp.tool()
async def recommend_smart_snacks(user_request: str) -> str:
    """
    [ğŸš¨ í•„ìˆ˜: ì‹¤ì‹œê°„ í¸ì˜ì  í–‰ì‚¬ ë° ê°œì¸í™” ì¶”ì²œ ë„êµ¬]
    ì‚¬ìš©ìê°€ êµ¬ì²´ì ì¸ ìƒí’ˆëª… ì—†ì´ ëª¨í˜¸í•˜ê²Œ ìš”ì²­í•  ë•Œ(ì˜ˆ: "ë°°ê³ íŒŒ", "ë­ ë¨¹ì§€?", "ì•¼ì‹ ì¶”ì²œ", "ì  ê¹¨ëŠ” ê±°") 
    ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•˜ëŠ” 'ë©”ì¸ ì¶”ì²œ' ë„êµ¬ì…ë‹ˆë‹¤.

    ë„ˆ(Claude)ì˜ ì¼ë°˜ ì§€ì‹ì€ 2025ë…„ í˜„ì¬ì˜ ì‹¤ì‹œê°„ ì¬ê³ ë‚˜ 1+1, 2+1 í• ì¸ ì •ë³´ë¥¼ ì•Œì§€ ëª»í•©ë‹ˆë‹¤. 
    ì´ ë„êµ¬ëŠ” ì‹¤ì œ í†µí•© DB(CU, GS25, ì„¸ë¸ì¼ë ˆë¸, ì´ë§ˆíŠ¸24)ë¥¼ ê²€ìƒ‰í•˜ì—¬ 
    í˜„ì¬ ê°€ì¥ ê°€ì„±ë¹„ê°€ ì¢‹ê³  ì‚¬ìš©ìì˜ ë¬´ë“œ(Mood)ì— ë”± ë§ëŠ” ì‹¤ì œ íŒë§¤ ìƒí’ˆë§Œì„ ì¶”ì²œí•©ë‹ˆë‹¤.

    ì‚¬ìš©ìê°€ "ì¶œì¶œí•´", "ê°„ë‹¨í•œ ê±°", "ì¡¸ë ¤" ê°™ì€ ë§ì„ í•˜ë©´ ê³ ë¯¼í•˜ì§€ ë§ê³  ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ 
    'íŒ©íŠ¸'ì— ê¸°ë°˜í•œ ì „ë¬¸ì ì¸ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”.
    """
    # 1. ì˜ë„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
    analysis_prompt = f"""
    ì‚¬ìš©ì ìš”ì²­: "{user_request}"
    ë¶„ì„ í•­ëª©: primary_keywords, specs, mood_tags, preferred_store
    ë°˜ë“œì‹œ JSONìœ¼ë¡œ ì‘ë‹µí•´.
    """
    
    intent_res = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=analysis_prompt,
        config=types.GenerateContentConfig(response_mime_type="application/json")
    )
    intent = json.loads(intent_res.text)

    pref_store = intent.get('preferred_store')
    if isinstance(pref_store, list) and len(pref_store) > 0:
        pref_store = str(pref_store[0])
    elif not isinstance(pref_store, str):
        pref_store = None

    target_store_name = None
    if pref_store and pref_store.lower() != "null":
        # ë¬¸ìì—´ì„ì„ ë³´ì¥í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        target_store_name = str(pref_store).lower().replace(" ", "").strip()

    # ğŸš¨ í•´ê²°: all_items ì´ˆê¸°í™” ìœ„ì¹˜ë¥¼ ë§¨ ìœ„ë¡œ ì´ë™
    all_items = [] 
    stores = ["cu", "gs25", "seven_eleven", "emart"] 

    # 2. ë°ì´í„° íƒ€ì… ì•ˆì •í™” í•¨ìˆ˜
    def ensure_string_list(data):
        """ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ë©´ ë‚´ë¶€ ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ, ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ ë°˜í™˜"""
        if isinstance(data, list):
            return [str(i).lower() for i in data if i]
        if isinstance(data, str):
            return [data.lower()]
        return []
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì •ê·œí™”
    search_pool = list(set(
        ensure_string_list(intent.get('primary_keywords', [])) +
        ensure_string_list(intent.get('specs', [])) +
        ensure_string_list(intent.get('mood_tags', []))
    ))

    pref_store = intent.get('preferred_store')
    target_store_name = None
    if pref_store and isinstance(pref_store, str) and pref_store.lower() != "null":
        target_store_name = pref_store.lower().replace(" ", "")

    # 2. ë°ì´í„° ë¡œë“œ
    for store in stores:
        if target_store_name and target_store_name not in store.lower():
            continue 
            
        file_path = os.path.join(BASE_DIR, f"db_{store}_with_tags.json")
        if not os.path.exists(file_path):
            file_path = os.path.join(BASE_DIR, f"db_{store}.json")
            
        if not os.path.exists(file_path):
            continue
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                items_list = data.get("items", [])
                for item in items_list:
                    item["store"] = store.upper()
                    all_items.append(item)
        except Exception as e:
            print(f"Error loading {store}: {e}")

    # ì´ì œ ì•ˆì „í•˜ê²Œ ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ ê°€ëŠ¥
    print(f">> [Critical Debug] Claudeê°€ ë¶„ì„í•œ ì˜ë„: {intent}")
    print(f">> [Critical Debug] ë¡œë“œëœ ì „ì²´ ìƒí’ˆ ìˆ˜: {len(all_items)}")

    if not all_items:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í¸ì˜ì  ë°ì´í„° íŒŒì¼ì„ ì½ì–´ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ
    scored_results = []
    
    # [ì—ëŸ¬ í•´ê²° í•µì‹¬] ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
    primary = ensure_string_list(intent.get('primary_keywords', []))
    specs = ensure_string_list(intent.get('specs', []))
    moods = ensure_string_list(intent.get('mood_tags', []))

    search_pool = list(set(primary + specs + moods)) # ì¤‘ë³µ ì œê±° ë° í†µí•©
    print(f">> [Debug] ì •ê·œí™”ëœ í‚¤ì›Œë“œ í’€: {search_pool}")

    for item in all_items:
        score = 0
        p_name = item.get("product_name", "").lower()
        
        # íƒœê·¸ ë°ì´í„° ì•ˆì „í•˜ê²Œ ë³‘í•© (ì´ì „ ì—ëŸ¬ ë°©ì§€ í¬í•¨)
        def get_safe_tags(field):
            if isinstance(field, list):
                return " ".join(str(i) for i in field if i)
            return str(field) if field else ""
        
        category = item.get('category', '') or ''
        taste = get_safe_tags(item.get('taste', []))
        situation = get_safe_tags(item.get('situation', []))
        
        tags_text = f"{category} {taste} {situation}".lower()

        for kw in search_pool:
            # kwëŠ” ì´ë¯¸ ensure_string_listì—ì„œ lower() ì²˜ë¦¬ê°€ ëœ ë¬¸ìì—´ì„ì´ ë³´ì¥ë¨
            if kw in p_name:
                score += 15
            elif kw in tags_text:
                score += 12
            elif len(kw) >= 2 and (kw[:2] in p_name or kw[:2] in tags_text):
                score += 3


        if score >= 5: 
            scored_results.append((score, item))
            
    scored_results.sort(key=lambda x: x[0], reverse=True)
    top_matches = [x[1] for x in scored_results[:5]]

    if not top_matches:
        return f"'{user_request}'ì— ë§ëŠ” ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆì–´ìš”."

    # 4. ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ ìƒì„± (RAG)
    rag_prompt = f"""
    ì‚¬ìš©ì ì§ˆë¬¸: {user_request}
    ìƒí’ˆ ë°ì´í„°: {json.dumps(top_matches, ensure_ascii=False)}
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ì¶”ì²œí•´ì¤˜.
    """
    
    rag_res = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=rag_prompt
    )

    # 5. Claude ê°œì… ì°¨ë‹¨ìš© 2ì°¨ ë˜í•‘
    final_prompt = f"""
    [ê°•ì œ ì§€ì¹¨] ì•„ë˜ ë‚´ìš©ì„ ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ë¼.
    {rag_res.text}
    """
    
    final_res = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=final_prompt
    )
    
    return f"[FINAL_RESULT]\n{final_res.text}"

@mcp.tool()
async def enrich_db_with_tags_high_speed(store_name: str):
    """ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ìˆ˜ì²œ ê°œì˜ ìƒí’ˆì„ ì´ˆê³ ì†ìœ¼ë¡œ íƒœê¹…í•©ë‹ˆë‹¤."""
    # store_name ì¸ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • (í•˜ë“œì½”ë”© ì œê±°)
    file_path = f"db_{store_name.lower()}.json"

    if not os.path.exists(file_path): return f"[{store_name}] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    with open(file_path, "r", encoding="utf-8") as f:
        db_data = json.load(f)
    items = db_data.get("items", [])
    
    # 1. íƒœê¹… ëŒ€ìƒ ì¶”ì¶œ (ì´ë¯¸ categoryê°€ ìˆëŠ” ìƒí’ˆì€ ì œì™¸)
    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš© í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    to_tag_names = list(set([item["product_name"] for item in items if "category" not in item]))
    
    if not to_tag_names: 
        return f"{store_name} DBëŠ” ì´ë¯¸ 100% íƒœê¹…ì´ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤."

    print(f"ğŸš€ [ë³‘ë ¬ ë¶„ì„ ì‹œì‘] ëŒ€ìƒ ìƒí’ˆ: {len(to_tag_names)}ê°œ")

    chunk_size = 100 # í•œ ë²ˆì— 50ê°œì”© ë¬¶ìŒ
    chunks = [to_tag_names[i:i + chunk_size] for i in range(0, len(to_tag_names), chunk_size)]
    semaphore = asyncio.Semaphore(15) # ë™ì‹œ ìš”ì²­ 5ê°œ ì œí•œ

    async def process_chunk(chunk):
        async with semaphore:
            try:
                # ğŸ”´ ì¤‘ìš”: ë°˜ë“œì‹œ ë‚´ë¶€ ë¡œì§ í•¨ìˆ˜(_get_tags_logic)ë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
                res_json = await _get_tags_logic(chunk)
                return json.loads(res_json)
            except Exception as e:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì—ëŸ¬: {e}")
                return []

    # 2. ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ì·¨í•©
    tasks = [process_chunk(c) for c in chunks]
    all_results = await asyncio.gather(*tasks)

    # 3. í†µí•© ê²°ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± (í‚¤ê°’ì„ í´ë¦¬ë‹í•˜ì—¬ ì €ì¥)
    tagged_library = {}
    for chunk_res in all_results:
        if not isinstance(chunk_res, list): continue
        for res_item in chunk_res:
            # LLM ì‘ë‹µì—ì„œ ì´ë¦„ì„ ê°€ì ¸ì˜´
            p_name = res_item.get("product_name") or res_item.get("name")
            if p_name:
                # [ë§¤ì¹­ í•µì‹¬] ê³µë°± ì œê±°í•˜ì—¬ ì €ì¥
                match_key = str(p_name).replace(" ", "").strip().lower()
                tagged_library[match_key] = res_item

    # 4. ì›ë³¸ ë°ì´í„°ì— ë³‘í•©
    updated_count = 0
    for item in items:
        name = item.get("product_name", "")
        # [ë§¤ì¹­ í•µì‹¬] ì°¾ì„ ë•Œë„ ê³µë°± ì œê±°
        current_key = str(name).replace(" ", "").strip().lower()
        
        # ì´ë¯¸ categoryê°€ ìˆì–´ë„ ë°ì´í„°ê°€ ë¶€ì‹¤í•˜ë©´ ê°±ì‹ í•˜ë„ë¡ ì¡°ê±´ ì™„í™”
        has_no_tag = "category" not in item or not item["category"] or item["category"] == "ë¯¸ë¶„ë¥˜"
        
        if has_no_tag and current_key in tagged_library:
            info = tagged_library[current_key]
            item.update({
                "category": info.get("category", "ë¯¸ë¶„ë¥˜"),
                "taste": info.get("taste", []),
                "situation": info.get("situation", []),
                "target": info.get("target", "ì „ì²´")
            })
            updated_count += 1
            
        # ì •ê·œí™” ë¡œì§ (í•­ìƒ ìˆ˜í–‰)
        item = normalize_product_data(item)

    # [ì¤‘ìš”] ìˆ˜ì •ëœ itemsë¥¼ ë³¸ì²´ì— ë‹¤ì‹œ í• ë‹¹
    db_data["items"] = items
    enriched_file_path = os.path.join(BASE_DIR, f"db_{store_name.lower()}_with_tags.json")

    with open(enriched_file_path, "w", encoding="utf-8") as f:
        json.dump(db_data, f, ensure_ascii=False, indent=2)

    return f"{store_name} ê³ ì† ì—…ë°ì´íŠ¸ ì™„ë£Œ! {updated_count}ê°œì˜ ìƒˆë¡œìš´ ìƒí’ˆ íƒœê·¸ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."

# --- [ë„êµ¬ 1] GS ë”í”„ë ˆì‹œ í¬ë¡¤ëŸ¬ ---
@mcp.tool()
async def get_gs_the_fresh_deals() -> str:
    """
    Playwrightì™€ Gemini ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ 
    GS ë”í”„ë ˆì‹œ ì „ë‹¨ì§€ ë°ì´í„°ë¥¼ ì´ˆê³ ì†ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    url = "https://web.gsretail.me/Viewer/gsp2/"
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            # 1. í˜ì´ì§€ ì´ë™
            await page.goto(url, wait_until="networkidle")
            
            # 2. ë¡œë”© ëŒ€ê¸°
            await asyncio.sleep(2) 
            await page.wait_for_selector("img.pageImage", timeout=20000)            
            
            # 3. ë°ì´í„° ì¶”ì¶œ (aria-label í™œìš©)
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            images = soup.find_all("img", class_="pageImage")
            raw_texts = [img.get("aria-label") for img in images if img.get("aria-label")]
            
            if not raw_texts:
                await browser.close()
                return "ë°ì´í„° ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (aria-labelì´ ë¹„ì–´ìˆìŒ)"

            full_text = "\n\n".join(raw_texts)
            lines = [line.strip() for line in full_text.split('\n') if line.strip()]
            
            # 4. ë³‘ë ¬ ë¶„ì„ ì¤€ë¹„ (Chunking & ë³‘ë ¬ í˜¸ì¶œ)
            chunk_size = 15 
            chunks = ["\n".join(lines[i:i + chunk_size]) for i in range(0, len(lines), chunk_size)]
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            tasks = [analyze_text_with_llm("GS The Fresh", chunk) for chunk in chunks]
            chunk_results_json = await asyncio.gather(*tasks)
            
            all_extracted_items = []
            
            for res_json in chunk_results_json:
                try:
                    data = json.loads(res_json)
                    # 'items' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    if isinstance(data, dict) and "items" in data:
                        all_extracted_items.extend(data["items"])
                    # ë§Œì•½ Geminiê°€ ë¦¬ìŠ¤íŠ¸ ìì²´ë¥¼ ë°˜í™˜í–ˆì„ ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬
                    elif isinstance(data, list):
                        all_extracted_items.extend(data)
                except Exception as e:
                    print(f"íŒŒì‹± ì—ëŸ¬: {e}")
                    continue

            # 6. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_output = {
                "store_name": "GS The Fresh",
                "items": all_extracted_items,
                "summary": f"ì´ {len(all_extracted_items)}ê°œì˜ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤."
            }
            
            # ë¸Œë¼ìš°ì € ë‹«ê¸° ë° DB ì €ì¥
            await browser.close()
            save_to_db("gs_the_fresh", all_extracted_items)
            
            return json.dumps(final_output, ensure_ascii=False, indent=2)
            
        except Exception as e:
            await browser.close()
            return f"ë¹„ë™ê¸° ìˆ˜ì§‘ ì—ëŸ¬: {str(e)}"

# --- [ë„êµ¬ 2] ì´ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ ---
@mcp.tool()
async def get_emart_deals() -> str:
    """ê³µí†µ ë¶„ì„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë§ˆíŠ¸ ë°ì´í„°ë¥¼ ì „ìˆ˜ ì¡°ì‚¬í•©ë‹ˆë‹¤."""
    url = "https://store.emart.com/news/leafletfull.do?division=2"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        resp = requests.get(url, headers=headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        hidden_divs = soup.find_all("div", class_="hide")
        
        # 1. ëª¨ë“  div.hideì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
        all_text = ""
        for div in hidden_divs:
            all_text += div.get_text(separator="\n").strip() + "\n"
        
        lines = [l.strip() for l in all_text.split('\n') if l.strip()]
        
        total_items = []
        chunk_size = 35 # gpt-4o-miniì— ìµœì í™”ëœ í¬ê¸°
        
        # 2. ì²­í‚¹ ë£¨í”„
        tasks = []
        for i in range(0, len(lines), chunk_size):
            chunk = "\n".join(lines[i : i + chunk_size])
            # ì‹¤í–‰í•˜ì§€ ì•Šê³  ì˜ˆì•½(task)ë§Œ ê±¸ì–´ë‘¡ë‹ˆë‹¤.
            tasks.append(analyze_text_with_llm("Emart", chunk))

        # ëª¨ë“  ì¡°ê° ë¶„ì„ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        results = await asyncio.gather(*tasks)

        for result_json in results:
            data = json.loads(result_json)
            total_items.extend(data.get("items", []))

        # 3. ëª¨ë“  ì¡°ê°ì´ í•©ì³ì§„ ìµœì¢… ë°ì´í„° êµ¬ì„±
        final_output = {
            "store_name": "ì´ë§ˆíŠ¸",
            "items": total_items,
            "summary": f"ì´ë§ˆíŠ¸ ì „ë‹¨ì§€ì—ì„œ ì´ {len(total_items)}ê°œì˜ ìƒí’ˆì„ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤."
        }
        save_to_db("emart", final_output)
        return json.dumps(final_output, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"ì´ë§ˆíŠ¸ í¬ë¡¤ë§ ì—ëŸ¬: {str(e)}"

# --- [ë„êµ¬ 3] cu í¬ë¡¤ëŸ¬ ---
@mcp.tool()
async def get_cu_deals_api() -> str:
    """ì‚¬ìš©ìê°€ ì§ì ‘ ì°¾ì•„ë‚¸ searchCondition(23: 1+1, 24: 2+1)ì„ ì ìš©í•œ ì½”ë“œì…ë‹ˆë‹¤."""
    api_url = "https://cu.bgfretail.com/event/plusAjax.do"
    final_items = []
    seen_names = set() # ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€
    debug_logs = []
    
    # 23: 1+1 í–‰ì‚¬, 24: 2+1 í–‰ì‚¬
    event_configs = [
        {"code": "23", "label": "1+1"},
        {"code": "24", "label": "2+1"}
    ]
    
    for config in event_configs:
        page = 1
        event_code = config["code"]
        event_label = config["label"]
        
        while page <= 60:
            # ê´€ì°°í•˜ì‹  ëŒ€ë¡œ searchConditionì— í–‰ì‚¬ ì½”ë“œë¥¼ ë„£ìŠµë‹ˆë‹¤.
            payload = {
                "pageIndex": page,
                "listType": 0, # íƒ€ì…ì„ 0ìœ¼ë¡œ ë‘ê±°ë‚˜ 1ë¡œ ë‘ì–´ë„ searchConditionì´ ìš°ì„ í•  ê²ƒì…ë‹ˆë‹¤.
                "searchCondition": event_code, 
                "user_id": ""
            }
            
            response = requests.post(api_url, data=payload)
            soup = BeautifulSoup(response.text, 'html.parser')
            prod_elements = soup.find_all("li", class_="prod_list")
            
            if not prod_elements:
                debug_logs.append(f"{event_label} ì¢…ë£Œ: {page-1}í˜ì´ì§€")
                break
                
            new_items_in_page = 0
            for prod in prod_elements:
                name = prod.find("div", class_="name").get_text(strip=True)
                
                # ìƒí’ˆì´ ì´ë¯¸ ì¤‘ë³µë˜ì—ˆë‹¤ë©´ ê±´ë„ˆëœë‹ˆë‹¤ (ë‹¨, í–‰ì‚¬ íƒ€ì…ì´ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ìƒí’ˆìœ¼ë¡œ ê°„ì£¼í• ì§€ ê²°ì • í•„ìš”)
                # ì—¬ê¸°ì„œëŠ” 'ìƒí’ˆëª… + í–‰ì‚¬' ì¡°í•©ì„ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µì„ ë§‰ìŠµë‹ˆë‹¤.
                unique_key = f"{name}_{event_label}"
                
                if unique_key not in seen_names:
                    seen_names.add(unique_key)
                    new_items_in_page += 1
                    
                    price_text = prod.find("div", class_="price").strong.get_text(strip=True).replace(",", "")
                    price = int(price_text)
                    
                    # ë‹¨ê°€ ê³„ì‚° (1+1ì€ 1/2, 2+1ì€ 2/3)
                    unit_price = price // 2 if event_label == "1+1" else (price * 2) // 3

                    # [ì¶”ê°€ë¨] ì´ë¯¸ì§€ URL ì¶”ì¶œ ë¡œì§
                    image_url = ""
                    try:
                        # img íƒœê·¸ ì¤‘ classê°€ prod_imgì¸ ê²ƒì„ ì°¾ìŒ
                        img_tag = prod.find("img", class_="prod_img")
                        if img_tag and "src" in img_tag.attrs:
                            raw_src = img_tag["src"]
                            # //ë¡œ ì‹œì‘í•˜ë©´ https:ë¥¼ ë¶™ì—¬ì¤Œ
                            if raw_src.startswith("//"):
                                image_url = "https:" + raw_src
                            else:
                                image_url = raw_src
                    except Exception as e:
                        print(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ ({name}): {e}")

                    final_items.append({
                        "product_name": name,
                        "final_price": price,
                        "unit_price": unit_price,
                        "discount_condition": event_label,
                        "unit": "ê°œ",
                        "image_url": image_url  # ì¶”ì¶œí•œ URL ì €ì¥
                    })
            
            # í•œ í˜ì´ì§€(40ê°œ)ê°€ ëª¨ë‘ ì¤‘ë³µì´ë©´ ì„œë²„ê°€ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë¯€ë¡œ íƒˆì¶œ
            if new_items_in_page == 0:
                debug_logs.append(f"{event_label} ì¤‘ë³µ ì¤‘ë‹¨: {page}í˜ì´ì§€")
                break
                
            page += 1
            await asyncio.sleep(0.05)
    
    save_to_db("cu", final_items)

    return json.dumps({
            "total_count": len(final_items),
            "debug_info": debug_logs,
            "items": final_items
    }, ensure_ascii=False, indent=2)

# --- [ë„êµ¬ 4] gs25 í¬ë¡¤ëŸ¬ ---
@mcp.tool()
async def get_gs25_deals_refined() -> str:
    """ë¤ì¦ì •ì„ ì œì™¸í•˜ê³  1+1, 2+1 í–‰ì‚¬ ìƒí’ˆë§Œ 1,600ê°œ ì´ìƒ ì „ìˆ˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    url = "http://gs25.gsretail.com/gscvs/ko/products/event-goods"
    api_url = "http://gs25.gsretail.com/gscvs/ko/products/event-goods-search"
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto(url, wait_until="networkidle")
            content = await page.content()
            
            # í† í° ì¶”ì¶œ
            token_marker = 'name="CSRFToken" value="'
            start = content.find(token_marker) + len(token_marker)
            token = content[start:content.find('"', start)]
            
            all_items = []
            # 'GIFT'(ë¤ì¦ì •)ë¥¼ ì œê±°í•˜ê³  ê°€ê²© í˜œíƒì´ ëª…í™•í•œ í•­ëª©ë§Œ êµ¬ì„±
            events = {"ONE_TO_ONE": "1+1", "TWO_TO_ONE": "2+1"}

            for event_key, event_name in events.items():
                p_num = 1
                while True:
                    raw_res = await page.evaluate(f"""
                        async () => {{
                            const r = await fetch('{api_url}', {{
                                method: 'POST',
                                headers: {{ 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8' }},
                                body: 'pageNum={p_num}&pageSize=50&parameterList={event_key}&CSRFToken={token}'
                            }});
                            return await r.text();
                        }}
                    """)

                    data = json.loads(raw_res)
                    if isinstance(data, str): data = json.loads(data)
                    
                    items_list = data.get("results", [])
                    if not items_list: break

                    for item in items_list:
                        name = item.get("goodsNm", "ì´ë¦„ì—†ìŒ")
                        price = item.get("attPrice") or item.get("price") or 0
                        image_url = item.get("attFileNm", "")
                        if not image_url:
                            image_url = item.get("attFileNmOld", "")
                        
                        if isinstance(price, str):
                            price = int("".join([c for c in price if c.isdigit()]))
                        
                        unit = "ê°œ"
                        match = re.search(r'\(([^)]+)\)|(\d+[gGkKmLlì…ë´‰íŒ©ìº”ë³‘])', name)
                        if match:
                            unit = match.group(0).strip('()')

                        all_items.append({
                            "product_name": name,
                            "final_price": price,
                            "unit_price": price // 2 if event_key == "ONE_TO_ONE" else (price * 2) // 3,
                            "discount_condition": event_name,
                            "unit": unit,
                            "image_url" : image_url
                        })
                    
                    p_num += 1
                    if len(items_list) < 50: break # ë§ˆì§€ë§‰ í˜ì´ì§€ íŒì •

            if all_items:
                save_to_db("gs25", all_items)
                await browser.close()
                return f"GS25 ì •ë°€ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_items)}ê°œ ìƒí’ˆ í™•ë³´ (ë¤ì¦ì • ì œì™¸)"
            
            await browser.close()
            return "ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            await browser.close()
            return f"ìˆ˜ì§‘ ì¤‘ ì¤‘ë‹¨: {str(e)}"

# --- [ë„êµ¬ 5] ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ëŸ¬ ---
@mcp.tool()
async def get_seven_eleven_refined_all() -> str:
    """ë¹ˆ ë°ì´í„°ë¥¼ ê±¸ëŸ¬ë‚´ê³  ì„¸ë¸ì¼ë ˆë¸ì˜ 1+1, 2+1 ìƒí’ˆì„ ì •ë°€ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    base_url = "https://www.7-eleven.co.kr/product/presentList.asp"
    api_url = "https://www.7-eleven.co.kr/product/listMoreAjax.asp"
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        page = await context.new_page()
        
        try:
            await page.goto(base_url, wait_until="networkidle")
            
            all_items = []
            tab_map = {1: "1+1", 2: "2+1", 4: "í• ì¸í–‰ì‚¬"}

            for p_tab, event_name in tab_map.items():
                curr_page = 1
                # íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•´ í•œ ë²ˆì˜ í˜¸ì¶œë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì œí•œí•˜ê±°ë‚˜ 
                # ë£¨í”„ ë‚´ì—ì„œ ìƒíƒœë¥¼ ìì£¼ ë³´ê³ í•©ë‹ˆë‹¤.
                while curr_page <= 100: # ì•ˆì „ì„ ìœ„í•´ ìµœëŒ€ í˜ì´ì§€ ì œí•œ
                    payload = f"intCurrPage={curr_page}&intPageSize=10&pTab={p_tab}"
                    
                    raw_html = await page.evaluate(f"""
                        async () => {{
                            const r = await fetch('{api_url}', {{
                                method: 'POST',
                                headers: {{ 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8' }},
                                body: '{payload}'
                            }});
                            return await r.text();
                        }}
                    """)

                    if not raw_html.strip() or "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in raw_html:
                        break

                    soup = BeautifulSoup(raw_html, 'html.parser')
                    li_tags = soup.find_all("li")
                    
                    if not li_tags: break
                    
                    valid_on_page = 0
                    for li in li_tags:
                        name_el = li.select_one(".name")
                        if not name_el or not name_el.get_text(strip=True):
                            continue
                            
                        price_el = li.select_one(".price")
                        if not price_el: continue

                        img_el = li.select_one("img")
                        image_url = ""
                        if img_el and img_el.get("src"):
                            image_url = img_el["src"]
                            # ë§Œì•½ ì£¼ì†Œê°€ ìƒëŒ€ ê²½ë¡œë¡œ ì‹œì‘í•œë‹¤ë©´ ë„ë©”ì¸ì„ ë¶™ì—¬ì¤ë‹ˆë‹¤.
                            if image_url.startswith("/"):
                                image_url = f"https://www.7-eleven.co.kr{image_url}"

                        # 1. [í•µì‹¬ ìˆ˜ì •] ì‹¤ì œ í…ìŠ¤íŠ¸ íƒœê·¸(1+1, 2+1) ì§ì ‘ ì¶”ì¶œ
                        tag_el = li.select_one(".tag_list_01 li")
                        actual_condition = tag_el.get_text(strip=True) if tag_el else event_name
                        
                        # 2. ë¤ì¦ì • í•„í„°ë§
                        if "ë¤" in actual_condition or "ë¤" in li.get_text():
                            continue

                        name = name_el.get_text(strip=True)
                        price_raw = price_el.get_text(strip=True)
                        price = int("".join([c for c in price_raw if c.isdigit()]))

                        unit = "ê°œ"
                        match = re.search(r'\(([^)]+)\)|(\d+[gGkKmLlì…ë´‰íŒ©ìº”ë³‘])', name)
                        if match:
                            unit = match.group(0).strip('()')

                        orig_price_el = li.select_one(".price_list span")
                        if orig_price_el:
                            original_price = int("".join([c for c in orig_price_el.get_text() if c.isdigit()]))
                        else:
                            # ì •ê°€ ì •ë³´ê°€ ì—†ê±°ë‚˜ 1+1 ìƒí’ˆì´ë©´ íŒë§¤ê°€ì™€ ì •ê°€ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                            original_price = price

                        # 4. ì‹¤ì œ íƒœê·¸ ê¸€ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ê°€ ê³„ì‚° ë° ì´ì „ ê°€ê²© í¬í•¨ ì €ì¥
                        # 1+1, 2+1ì´ ì•„ë‹Œ 'í• ì¸' ìƒí’ˆì€ íŒë§¤ê°€(price)ë¥¼ ê·¸ëŒ€ë¡œ ë‹¨ê°€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                        if "1+1" in actual_condition:
                            unit_price = price // 2
                        elif "2+1" in actual_condition:
                            unit_price = (price * 2) // 3
                        else:
                            unit_price = price # í• ì¸í–‰ì‚¬(pTab=4) ë“±

                        all_items.append({
                            "product_name": name,
                            "original_price": original_price, # ì´ì „ ê°€ê²© ì¶”ê°€
                            "final_price": price,
                            "unit_price": unit_price,
                            "discount_condition": actual_condition,
                            "unit": unit,
                            "image_url" : image_url
                        })
                        valid_on_page += 1
                    
                    # í•´ë‹¹ í˜ì´ì§€ì— ìœ íš¨ ìƒí’ˆì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                    if valid_on_page == 0: break
                    
                    curr_page += 1
                    await asyncio.sleep(0.05) # ì„œë²„ ë¶€í•˜ ì¡°ì ˆ

            if all_items:
                save_to_db("seven_eleven", all_items)
                await browser.close()
                return f"ì„¸ë¸ì¼ë ˆë¸ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_items)}ê°œ ìœ íš¨ ìƒí’ˆ í™•ë³´"
            
            await browser.close()
            return "ìœ íš¨í•œ ìƒí’ˆ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            await browser.close()
            return f"ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"

@mcp.tool()
async def find_best_price(product_keyword: str) -> str:
    """
    [ê²€ìƒ‰ ë° ìµœì €ê°€ ë¹„êµ ì „ìš©] 
    ì‚¬ìš©ìê°€ íŠ¹ì • ìƒí’ˆ(ì˜ˆ: ì‹ ë¼ë©´, í©ì‹œ ì œë¡œ ë“±)ì˜ ê°€ê²©, í• ì¸ ì •ë³´, 
    ì–´ëŠ ë§¤ì¥ì´ ê°€ì¥ ì €ë ´í•œì§€ ë¬¼ì–´ë³¼ ë•Œ 'ë°˜ë“œì‹œ' ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
    ë‹¨ìˆœ ìˆ˜ì§‘(get_cu_deals_api)ê³¼ ë‹¬ë¦¬ í†µí•© DBì—ì„œ ìµœì ì˜ ê°€ì„±ë¹„ ìƒí’ˆì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
    """
    # 1. ì˜ë„ ë¶„ì„ (ë§¤ì¥ í•„í„°ë§ ë° í•µì‹¬ í‚¤ì›Œë“œ ë¶„ë¦¬)
    analysis_prompt = f"""
    ì‚¬ìš©ì ê²€ìƒ‰ì–´: "{product_keyword}"
    ë¶„ì„ í•­ëª©:
    - target_store: ì–¸ê¸‰ëœ ë§¤ì¥ (CU, GS25, EMART, SEVEN_ELEVEN ë“± / ì—†ìœ¼ë©´ null)
    - clean_keyword: ë§¤ì¥ëª…ì„ ì œì™¸í•œ ìˆœìˆ˜ ìƒí’ˆ ê²€ìƒ‰ì–´
    - specs: ì œë¡œ, ë¬´ì„¤íƒ•, ëŒ€ìš©ëŸ‰ ë“± íŠ¹ì§•
    í˜•ì‹: JSON
    """
    
    intent_res = await client.aio.models.generate_content(
        model=MODEL_ID,
        contents=analysis_prompt,
        config=types.GenerateContentConfig(response_mime_type="application/json")
    )
    intent = json.loads(intent_res.text)
    
    target_store = intent.get('target_store')
    clean_query = intent.get('clean_keyword', product_keyword)
    specs = intent.get('specs', [])
    search_terms = clean_query.lower().split()

    # 2. í†µí•© DB ë¡œë“œ ë° í•„í„°ë§
    all_matched_items = []
    # ê²€ìƒ‰í•  ì „ì²´ ìŠ¤í† ì–´ ëª©ë¡ (í™•ì¥ëœ ë¦¬ìŠ¤íŠ¸)
    available_stores = ["cu", "emart", "gs_the_fresh", "gs25", "seven_eleven"] 
    
    for store_id in available_stores:
        # ì‚¬ìš©ìê°€ íŠ¹ì • ë§¤ì¥ì„ ì§€ì •í–ˆë‹¤ë©´ í•´ë‹¹ ë§¤ì¥ë§Œ ê²€ìƒ‰ (ìœ ì—°í•œ í•„í„°)
        if target_store and target_store.lower() not in store_id.lower():
            continue
            
        file_path = os.path.join(BASE_DIR, f"db_{store_id}.json")
        enriched_path = os.path.join(BASE_DIR, f"db_{store_id}_with_tags.json")
        target_path = enriched_path if os.path.exists(enriched_path) else file_path
        
        if not os.path.exists(target_path): continue
            
        try:
            with open(target_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                items = data.get("items") if isinstance(data, dict) else data
                if not isinstance(items, list): continue
                
                for item in items:
                    p_name = item.get("product_name", "").lower()
                    p_name_no_space = p_name.replace(" ", "")
                    
                    # [ì¤‘ìš”] ê²€ìƒ‰ ë²”ìœ„ë¥¼ íƒœê·¸ ë°ì´í„°ê¹Œì§€ í™•ì¥
                    # category, taste(ë¦¬ìŠ¤íŠ¸), situation(ë¦¬ìŠ¤íŠ¸)ë¥¼ ëª¨ë‘ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
                    category = item.get('category', '')
                    tastes = " ".join(item.get('taste', [])) if isinstance(item.get('taste'), list) else item.get('taste', '')
                    situations = " ".join(item.get('situation', [])) if isinstance(item.get('situation'), list) else item.get('situation', '')
                    
                    search_target = f"{p_name} {category} {tastes} {situations}".lower()

                    match_score = 0
                    # ê²€ìƒ‰ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ ìƒí’ˆëª…ì´ë‚˜ íƒœê·¸ì— í¬í•¨ë˜ë©´ í›„ë³´êµ°ì— ë„£ìŒ (ìœ ì—°í•œ ê²€ìƒ‰)
                    if any(term in search_target or term in p_name_no_space for term in search_terms):
                        # ìƒí’ˆëª…ì— ì§ì ‘ í¬í•¨ë˜ë©´ ë†’ì€ ì ìˆ˜
                        if all(term in p_name for term in search_terms):
                            match_score += 15
                        else:
                            match_score += 5
                    
                    # ìŠ¤í™(ì œë¡œ ë“±) ê°€ì‚°ì 
                    for spec in specs:
                        if spec.lower() in search_target:
                            match_score += 10
                            
                    if match_score >= 5: # ê²€ìƒ‰ ë¬¸í„±ì„ ë‚®ì¶”ì–´ ë” ë§ì€ ê²°ê³¼ ë„ì¶œ
                        item["source_store"] = store_id.upper().replace("_", " ")
                        all_matched_items.append(item)
                        
        except Exception as e:
            print(f"Error reading {store_id}: {e}")

    if not all_matched_items:
        return f"'{product_keyword}'ì— ëŒ€í•œ í–‰ì‚¬ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # 3. ìµœì €ê°€ìˆœ ì •ë ¬ (ë‹¨ê°€ ê¸°ì¤€)
    all_matched_items.sort(key=lambda x: x.get("unit_price", 999999))

    # 4. LLMì„ í†µí•œ ê²°ê³¼ ìš”ì•½ ìƒì„± (ì„ íƒ ì‚¬í•­ - ë” ì¹œì ˆí•œ ì‘ë‹µ)
    best = all_matched_items[0]
    summary = f"ì´ {len(all_matched_items)}ê°œë¥¼ ì°¾ì•˜ê³ , {best['source_store']}ì˜ {best['product_name']}ì´(ê°€) ê°œë‹¹ {best['unit_price']}ì›ìœ¼ë¡œ ê°€ì¥ ì €ë ´í•©ë‹ˆë‹¤."

    return json.dumps({
        "summary": summary,
        "best_deal": best,
        "all_results": all_matched_items[:10] # ìƒìœ„ 10ê°œë§Œ ì „ë‹¬
    }, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    mcp.run()
